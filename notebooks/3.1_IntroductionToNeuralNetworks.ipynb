{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3.1_IntroductionToNeuralNetworks.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mFQ2JNZn8oIm"},"source":["<img src=https://brand.uark.edu/_resources/images/UA_Logo_Horizontal.jpg width=\"400\" height=\"96\">\n","\n","###_Artificial intelligence for image processing and analysis._"]},{"cell_type":"markdown","metadata":{"id":"8g_Y-BQS8vsu"},"source":["# Notebook 3.1 Introduction to Neural Networks\n","---\n","##### The purpose of this notebook is to introduce the idea of neural networks and how they are created.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jpb19CYvFkL8"},"source":["# How do computers learn?\n","---\n","##### Traditionally, computers are designed to only follow instructions that are assigned to it. For us to tell a computer what to do, we first generate code that is written so we (humans) can read it. This code then get _compiled_ into machine code, which a computer can then execute. In the realm of computers, information is stored in _bytes_ of data, which contain only 0's and 1's, and only has the information that we give to it. So is there a way that computers can learn?\n","##### There is a quote that Tom M. Mitchell, one of the first people to really ponder this question, that explains how a computer could think or learn:\n","> \"A computer program is said to learn from experience _E_ with respect to some class of tasks _T_ and performance measure _P_ if its performance at tasks in _T_, as measured by _P_, improves with experience _E_.\" -Tom M. Mitchell\n","\n","##### This question led to the birth of a topic within Computer Science call _Machine Learning_, or sometime referred to as _Artificial Intelligence (AI)_.\n","##### Although these terms are sometime used  interchangably, is it important to know that they are different.\n"]},{"cell_type":"markdown","metadata":{"id":"CjXe3mvGImkj"},"source":["# Machine Learning versus Artificial Intelligence\n","---\n","##### Imagine you decide to play a game with a friend. You have never heard of this game, but it is your friend's favorite. Your friend tells you to reach the end of the game, but does not tell you how to reach the end. Your friend then does one of two things:\n","1. Explains the rules of the game, and how to reach the end, or\n","2. Does not tell you any of the rules, but rather tells you if you can or can not do something whenever you try it.\n","\n","##### Upon completing the game, depending on which of the two scenarios your friend decides to do, you will either:\n","1. Get to the intended solution of the game, but not learn anything new since you were told the rule set, or\n","2. Potentially not arrive at the intended solution, but learn a lot about the rule set along the way. \n","\n","##### These two scenarios can be easily translated to the topic of _Articial Intelligence_. The first scenario is synonymous to _Machine Learning_, while the second is similar to _Deep Learning_.\n","##### In this analogy, the set of rules that is known by your friend is called a _dataset_, and an untrained player of the game is a _model_. \n","##### Finally, when a _model_ has been trained to reach some solution, then the _model_ can be implemented, and is some type of _artificial intelligence_.\n","##### **In this module, we will only cover _deep learning_, but both are equally as interesting and have their own pros and cons.** \n"]},{"cell_type":"markdown","metadata":{"id":"_TQnim4zR9vd"},"source":["# Neurons and Perceptrons\n","---\n","##### To train a model with _deep learning_, we must first consider the small unit of learning and thinking in the human brain: the _neuron_.\n","<img class=\"center\" src=\"https://training.seer.cancer.gov/images/anatomy/nervous/neuron-structure.jpg\" alt=\"Illustration of a neuron\" width=\"561\" height=\"351\">\n","\n","##### In the most basic form, the neuron consists of three sections: the dendrites, cell body, and axon.\n","##### The dendrites (or _inputs_) of the neuron is where electrical signals are recieved, which get passed on (or _propogated_) through the cell body and the axon (or _output_). The axon of a neuron connects to the dendrites of some number of neurons, and during the learning process, these connections (or _weights_) are either strengthened or weakened. This results in a large complex network that is able to think and continually learn. \n","##### Mathematically, we can describe the output of a neuron as a summation of each input multiplied by its weight: \n","> $ \\text{output} = \\sum_{i=1}^{n} weight_i * input_i $ where $n$ is the total number of inputs\n","##### Typically, the mathematical sum that is calculated is passed through an additional _activation function_, which is used to represent whether or not the neuron propogates the signal it has received. Here is a simple activation function that uses a threshold value of 0:\n","\n","> $ \\text{activation_output} =\n","  \\begin{cases}\n","    0  & \\quad \\text{if activation_input} < 0 \\\\\n","    1  & \\quad \\text{if activation_input} \\geq 0\n","  \\end{cases}\n","$\n","\n","##### For deep learning, this mathematical equation of inputs and outputs can be represented as the base unit of a neural network: the _perceptron_. \n","<img width=\"572\" height=\"318\" src=\"https://pythonmachinelearning.pro/wp-content/uploads/2017/09/Single-Perceptron.png.webp\">\n","\n","##### The following code chunk shows an example of a perceptron.\n"]},{"cell_type":"code","metadata":{"id":"kn1ZI-vLKdu5"},"source":["# Import required packages\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","# Generate weights that range between 0 and 1 for some number of inputs\n","# The range of values used for the weights is somewhat arbitrary\n","number_of_inputs = 7\n","weights = np.random.random(number_of_inputs)\n","output_weight = np.random.random(1)\n","\n","# Make a nice looking figure for inputs\n","fig = plt.figure(figsize=(20,10))\n","inputs_figure = plt.scatter(np.zeros(number_of_inputs),range(number_of_inputs), s=2000, c='#ffffff', edgecolors='#000000', alpha=1, zorder=10)\n","weights_figure = plt.plot([np.zeros(number_of_inputs),np.ones(number_of_inputs)],[np.arange(0,number_of_inputs),np.ones(number_of_inputs)*((number_of_inputs-1)/2)],zorder=1)\n","weight_colors = plt.cm.coolwarm(weights)\n","for i in range(number_of_inputs):\n","  weights_figure[i].set_color(weight_colors[i,:])\n","sum_figure = plt.scatter(1,(number_of_inputs-1)/2, s=2000, c='#ffffff', edgecolors='#000000', alpha=1, zorder=10)\n","sum_line = plt.plot([1,2],[(number_of_inputs-1)/2,(number_of_inputs-1)/2],zorder=1)\n","\n","sum_line[0].set_color(plt.cm.coolwarm(weights)[0,:])\n","ax = plt.gca()\n","ax.set_title('Example of perceptron with inputs on left, summation in middle, and output on right, before activation function',fontsize='x-large')\n","ax.set_axis_off()\n","cbar = fig.colorbar(plt.cm.ScalarMappable(norm = matplotlib.colors.Normalize(vmin=-1, vmax=1), cmap='coolwarm'))\n","cbar.set_label('Weight of connections',fontsize='x-large')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Z-uW0lA1btN"},"source":["# Neural Networks\n","---\n","##### Similar to a vast network of neurons in the human brain, perceptrons can be added in both _series_ and _parallel_ to develop complex neural networks. If the neural network only consists of perceptrons (more about this later), then the network is called a _multi-layer perceptron (MLP)_. \n","##### In terms of a MLP, perceptrons that are in _parallel_ form a _layer_ of the network, and perceptrons in _series_ use the outputs of all perceptrons in the previous layer as their inputs. This helps form a complex, interconnected neural network that can be trained to complete some task.\n","##### The following code chunk showcases an example of the MLP architecture, where you can modify some of the parameters of the neural network."]},{"cell_type":"code","metadata":{"cellView":"form","id":"3-dj4ftuLNBK"},"source":["# Import required packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define neural network parameters\n","#@title Network parameters: { run : \"auto\"}\n","number_of_layers = 5 #@param {type:\"slider\", min:1, max:5, step:1}\n","number_of_perceptrons_per_layer = 10 #@param {type:\"slider\", min:1, max:10, step:1}\n","number_of_inputs = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n","number_of_outputs = 1 #@param {type:\"slider\", min:1, max:5, step:1}\n","\n","# Generate random weights\n","input_weights = np.random.random((number_of_inputs,number_of_perceptrons_per_layer))\n","layer_weights = np.random.random((number_of_perceptrons_per_layer,number_of_perceptrons_per_layer,number_of_layers-1))\n","output_weights = np.random.random((number_of_perceptrons_per_layer,number_of_outputs))\n","\n","# Create figure\n","fig = plt.figure(figsize=(20,10))\n","\n","# Plot perceptrons\n","inputs_perceptrons = plt.scatter(np.zeros(number_of_inputs),range(number_of_inputs), s=1000, c='#ffffff', edgecolors='#000000', alpha=1, zorder=10)\n","count = 1\n","layer_perceptrons = []\n","layer_offset = ((number_of_perceptrons_per_layer-number_of_inputs)/2)\n","for i in range(number_of_layers):\n","  layer_perceptrons.append(plt.scatter(np.ones(number_of_perceptrons_per_layer)*count,np.arange(number_of_perceptrons_per_layer)-layer_offset, s=1000, c='#ffffff', edgecolors='#000000', alpha=1, zorder=10))\n","  count += 1\n","output_offset = ((number_of_outputs-number_of_inputs)/2)\n","outputs_perceptrons = plt.scatter(np.ones(number_of_outputs)*count,np.arange(number_of_outputs)-output_offset, s=1000, c='#ffffff', edgecolors='#000000', alpha=1, zorder=10)\n","\n","# Plot weights\n","for i in range(number_of_inputs):\n","  for j in range(number_of_perceptrons_per_layer):\n","    temp_color = plt.cm.coolwarm(input_weights[i,j])\n","    plt.plot([0,1],[i,j-layer_offset],zorder=1,c=temp_color)\n","\n","count = 1\n","for i in range(number_of_layers-1):\n","  for j in range(number_of_perceptrons_per_layer):\n","    for k in range(number_of_perceptrons_per_layer):\n","      temp_color = plt.cm.coolwarm(layer_weights[k,j,i])\n","      plt.plot([count,count+1],[j-layer_offset,k-layer_offset],zorder=1,c=temp_color)\n","  count += 1\n","\n","for i in range(number_of_perceptrons_per_layer):\n","  for j in range(number_of_outputs):\n","    temp_color = plt.cm.coolwarm(output_weights[i,j])\n","    plt.plot([count,count+1],[i-layer_offset,j-output_offset],zorder=1,c=temp_color)    \n","\n","ax = plt.gca()\n","ax.set_title(\"Example of MLP with \" + str(number_of_inputs) + \" inputs, \" + str(number_of_layers) + \" layers, \" + str(number_of_perceptrons_per_layer) + \" perceptrons per layer, and \" + str(number_of_outputs) + \" outputs.\",fontsize='x-large')\n","ax.set_axis_off()\n","cbar = fig.colorbar(plt.cm.ScalarMappable(norm = matplotlib.colors.Normalize(vmin=-1, vmax=1), cmap='coolwarm'))\n","cbar.set_label('Weight of connection',fontsize='x-large')"],"execution_count":null,"outputs":[]}]}