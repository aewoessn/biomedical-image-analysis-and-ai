{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.3_ConvolutionalNeuralNetworks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFQ2JNZn8oIm"
      },
      "source": [
        "<img src=https://brand.uark.edu/_resources/images/UA_Logo_Horizontal.jpg width=\"400\" height=\"96\">\n",
        "\n",
        "###_Biomedical Image Analysis & Artificial Intelligence_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g_Y-BQS8vsu"
      },
      "source": [
        "# Notebook 3.3 Convolutional Neural Networks\n",
        "---\n",
        "##### The purpose of this notebook is to motivate and discuss the use of convolutional neural networks (CNNs) for biomedical applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44Kj2s6vPtIm"
      },
      "source": [
        "### Required packages\n",
        "---\n",
        "##### **_NOTE: This notebook is accelerated with the use of GPU hardware, but is not required. please refer to notebook 2.4_ParallelProcessing if you need a refresher on how to do this._**\n",
        "##### **_Run this code chunk first. If you encounter an error when trying to run code chunks in this notebook, then first try re-running this chunk._**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq84n-cRL-aP"
      },
      "source": [
        "# Import all of the necessary packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cZkzi7Zhtwt"
      },
      "source": [
        "# Load the MNIST digits dataset\n",
        "import os\n",
        "\n",
        "# Check to see if the training file exists...\n",
        "filename = \"/content/MNIST/processed/training.pt\"\n",
        "if not os.path.isfile(filename):\n",
        "  # ...and if it does not, then download it,...\n",
        "  !gdown --id 1jNqLo0LBYrPcGzaDBBA60sqGlclS_nX_\n",
        "\n",
        "  # ...check to see if the destination path exists,...\n",
        "  if not os.path.isdir(\"/content/MNIST/processed/\"):\n",
        "    os.makedirs(\"/content/MNIST/processed/\")\n",
        "\n",
        "  # ...and move the file.\n",
        "  os.rename(\"/content/training.pt\",filename)\n",
        "\n",
        "# Check to see if the test file exists...\n",
        "filename = \"/content/MNIST/processed/test.pt\"\n",
        "if not os.path.isfile(filename):\n",
        "  # ...and ff it does not, then download it,...\n",
        "  !gdown --id 178t2_ul2VvnAHuClPhlE27qHkpE5I8S1\n",
        "\n",
        "  # ...and move the file.\n",
        "  os.rename(\"/content/test.pt\",filename)\n",
        "\n",
        "# Create the dataset\n",
        "mnist_data = torchvision.datasets.MNIST(root='/content/', transform=T.ToTensor())\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miolqLuELujB"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "---\n",
        "##### In the previous notebook, we saw that a deep learning MLP model can be trained to predict a number based on an image of that handwritten digit. This can be very useful for many applications, but an MLP does have an important downfall: spatial features.\n",
        "##### At the end of the last notebook, we saw that the weights of a trainined MLP tended to be much larger in the center of the input image where the digits are centered, and we asked these two questions:\n",
        "1. Do you think we would get different results?\n",
        "2. How do you think we can get around this issue?\n",
        "\n",
        "##### To answer the first question, we would expect to get mostly incorrect predictions for the digit in the image, since the network was not trained on images where the digit lies near the edge on the input image.\n",
        "##### One work around for this issue is to _augment_ the dataset by translating (or moving around), rotating, or even scaling the digits in the images. However, there are an infinite number of combinations that we could do for augmentation, resulting in an MLP that will not always be correct.\n",
        "##### Another thing we can do, however, is _scan_ the entire image and _extract features_ that are attributed to each digit.  As we learned about in the previous notebooks, this can be accomplished with images through _convolution_. Using convolution would result in a more robust network that could handle many more cases where handwritten digits or other features are located away from the center of the image. These unique features in the image can be highlighted through the use of many small _kernels_ (or _filters_) that are initially randomly chosen, but we could update and adjust via deep learning. Additionally, features can be _pooled_ together to decide which features matter most, which is done in a _pooling layer_. This kind of feature extraction network is called a _convolutional neural network (CNN)_.\n",
        "##### The code chunk below show an example of one convolutional _layer_ that can be used in a CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A27-YRMKQDHw",
        "cellView": "form"
      },
      "source": [
        "# Define some parameters for a convolutional layer\n",
        "#@title Parameters for the convolutional layer. {run:\"auto\"}\n",
        "number_of_filters = 9 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "filter_size = 5 #@param {type:\"slider\", min:3, max:11, step:2}\n",
        "\n",
        "# Get the input image\n",
        "input_image = mnist_data.data[0,:,:].unsqueeze(0).unsqueeze(0).float()\n",
        "\n",
        "# Define the single convolutional layer\n",
        "conv_layer = nn.Conv2d(in_channels=1,out_channels=number_of_filters, kernel_size=filter_size)\n",
        "conv_layer.requires_grad_(False)\n",
        "weights = conv_layer.weight\n",
        "\n",
        "# Show the weights (filters)\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ncols = 5\n",
        "if weights.shape[0] % ncols == 0: \n",
        "  nrows = weights.shape[0] // ncols\n",
        "else: \n",
        "  nrows = (weights.shape[0] // ncols) + 1\n",
        "\n",
        "count = 0\n",
        "dim_size = weights.shape[2]\n",
        "full_image = np.zeros((dim_size*nrows,dim_size*ncols))\n",
        "for i in range(nrows):\n",
        "  plt.plot([-0.5,dim_size*ncols],[i*filter_size-0.5,i*filter_size-0.5],'r-')\n",
        "  for j in range(ncols):\n",
        "    plt.plot([j*filter_size-0.5,j*filter_size-0.5],[-0.5,dim_size*nrows],'r-')\n",
        "    if count < weights.shape[0]:\n",
        "      full_image[i*dim_size:i*dim_size+dim_size,j*dim_size:j*dim_size+dim_size] = weights[count,0,:,:].cpu()\n",
        "      count+=1\n",
        "plt.plot([-0.5,dim_size*ncols],[nrows*filter_size-0.5,nrows*filter_size-0.5],'r-')\n",
        "plt.plot([ncols*filter_size-0.5,ncols*filter_size-0.5],[-0.5,dim_size*nrows],'r-')\n",
        "plt.imshow(full_image)\n",
        "plt.gca().set_title('Layer 1 filters',fontsize='x-large');\n",
        "plt.gca().set_axis_off()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh6z8VDOVHk9"
      },
      "source": [
        "### Fully connected layers\n",
        "---\n",
        "##### Although convolutional layers are very powerful at extracting features in an image, they are unable to predict a classification like an MLP can. To complete a CNN the predicts a digit based on the features that were extracted, we can add activation functions after each convolutional layer, as well as an MLP network to the end of the feature extraction portion of a CNN.\n",
        "##### Similar to the previous notebook, the model is first initialized with random weights, meaning that initially the output will demonstrate that the model is not confident at guessing the digit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwEebPRMVvF6"
      },
      "source": [
        "# Define the network architecture\n",
        "filter_size = 3\n",
        "number_of_filters = 32\n",
        "\n",
        "test_model = nn.Sequential(OrderedDict([\n",
        "    ('conv1', nn.Conv2d(in_channels=1,out_channels=number_of_filters, kernel_size=filter_size)),\n",
        "    ('relu1',nn.ReLU()),\n",
        "    ('mp1',nn.MaxPool2d(kernel_size=2)),\n",
        "    ('flat',nn.Flatten(start_dim=1,end_dim=-1)),\n",
        "    ('fc1',nn.Linear(5408,100)),\n",
        "    ('relu2',nn.ReLU()),\n",
        "    ('fc2',nn.Linear(100,10)),\n",
        "]))\n",
        "clear_output()\n",
        "\n",
        "# Get the first image in the MNIST dataset\n",
        "input_image = mnist_data.data[0,:,:].unsqueeze(0).unsqueeze(0).float() / 255.\n",
        "\n",
        "# Compute the forward pass of the image\n",
        "output = test_model.forward(input_image)\n",
        "\n",
        "# Get the probabilities by passing the output through a softmax\n",
        "prob = torch.softmax(output,1)*100\n",
        "\n",
        "# Show the image\n",
        "fig = plt.Figure()\n",
        "img = plt.imshow(input_image.squeeze(0).squeeze(0))\n",
        "ax = plt.gca()\n",
        "ax.set_title('Input image',fontsize='x-large');\n",
        "plt.show()\n",
        "\n",
        "# Generate a report\n",
        "print('\\nNumber - Probability [%]\\n')\n",
        "for i in range(10):\n",
        "  print(str(i) + '   -    ' + str(prob[0,i].detach().numpy()) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sgdlj9gYPuq"
      },
      "source": [
        "### Training a CNN\n",
        "---\n",
        "##### The _training loop_ for a CNN has exactly the same steps as for a MLP:\n",
        "1. Forward pass\n",
        "2. Loss calculation\n",
        "3. Back-propogation\n",
        "4. Gradient descent\n",
        "\n",
        "##### For a CNN, the weights from an MLP are synonymous with each _pixel_ of each filter in a convolutional layer, and can be adjusted in a similar fashion.\n",
        "\n",
        "##### Since the CNN is trained on a single image, the expected output from this code chunk is for the CNN to have slightly more confidence in the correct digit. You can also see that the filters in the CNN have changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQsT0OkYZW9h"
      },
      "source": [
        "# Define which loss function we will be using\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Get the correct label for the image\n",
        "input_label = mnist_data.targets[0].unsqueeze(0)\n",
        "\n",
        "# Calculate the loss based on the output from the model\n",
        "model_loss = loss(output,input_label)\n",
        "\n",
        "# Back-propogate the loss function through the network\n",
        "model_loss.backward()\n",
        "\n",
        "# Specify the optimizer (or gradient descent algorithm)\n",
        "optimizer = torch.optim.Adam(test_model.parameters())\n",
        "\n",
        "# Gradient descent iteration\n",
        "optimizer.step()\n",
        "\n",
        "# Compute the forward pass of the image\n",
        "output = test_model.forward(input_image)\n",
        "\n",
        "# Get the probabilities by passing the output through a softmax\n",
        "prob = torch.softmax(output,1)*100\n",
        "\n",
        "# Show the image\n",
        "fig = plt.Figure()\n",
        "img = plt.imshow(input_image.squeeze(0).squeeze(0))\n",
        "ax = plt.gca()\n",
        "ax.set_title('Input image',fontsize='x-large');\n",
        "plt.show()\n",
        "\n",
        "# Generate a report\n",
        "print('\\nNumber - Probability [%]\\n')\n",
        "for i in range(10):\n",
        "  print(str(i) + '   -    ' + str(prob[0,i].detach().numpy()) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EktY1JgGcNB-"
      },
      "source": [
        "# Making CNNs deep\n",
        "---\n",
        "##### In general, the more layers to a model, the more _deep_ the model is said to be. For CNNs, a deeper convolutional layer will be able to extract _higher level_ (more descriptive) features. \n",
        "#####  In the code chunk below, we train a CNN with 3 convolutional layers on the digits dataset like before. **_A GPU is highly recommended for training a CNN, but should only take a few minutes without a GPU._**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy2tycoOejQf",
        "cellView": "form"
      },
      "source": [
        "#@title --- Hidden code (double-click to show code) ---\n",
        "\n",
        "# Give some context of what is going on\n",
        "print('Preparing the network and dataset...')\n",
        "\n",
        "# Define the network architecture\n",
        "model = nn.Sequential(OrderedDict([\n",
        "    ('conv1', nn.Conv2d(in_channels=1,out_channels=32, kernel_size=3)),\n",
        "    ('relu1',nn.ReLU()),\n",
        "    ('mp1',nn.MaxPool2d(kernel_size=2)),\n",
        "    ('flat',nn.Flatten(start_dim=1,end_dim=-1)),\n",
        "    ('fc1',nn.Linear(5408,100)),\n",
        "    ('relu2',nn.ReLU()),\n",
        "    ('fc2',nn.Linear(100,10)),\n",
        "]))\n",
        "try:\n",
        "  model = model.cuda()\n",
        "  canUseGPU = True\n",
        "except:\n",
        "  print('Warning: The GPU has not been enabled. Training may take longer than expected.')\n",
        "  canUseGPU = False\n",
        "\n",
        "# Load the MNIST digits dataset\n",
        "batchsize = 10\n",
        "data_loader = torch.utils.data.DataLoader(mnist_data,batch_size=batchsize,shuffle=True)\n",
        "\n",
        "# Specify the number of epochs\n",
        "number_of_epochs = 3\n",
        "\n",
        "# Pull out a small subset of the dataset that will be used to benchmarking purposes\n",
        "test_images = mnist_data.data[[1,3,5,7,9,0,13,15,17,4],:,:].float().unsqueeze(1) / 255.\n",
        "\n",
        "if canUseGPU:\n",
        "  test_images = test_images.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "  test_output = model.forward(test_images).cpu()\n",
        "  test_prob = torch.transpose(torch.softmax(test_output,1)*100,0,1)\n",
        "\n",
        "print('...Done')\n",
        "\n",
        "# Plot the results\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.imshow(test_prob)\n",
        "ax = plt.gca()\n",
        "plt.xticks([])\n",
        "secax = ax.secondary_xaxis('top')\n",
        "secax.set_xticks(np.arange(0,10))\n",
        "secax.set_xlabel('Predicted digit',fontsize='x-large')\n",
        "plt.yticks(np.arange(0,10))\n",
        "ax.set_ylabel('Actual digit',fontsize='x-large')\n",
        "plt.clim(0,100)\n",
        "cbar = plt.colorbar()\n",
        "cbar.set_label('Confidence in prediction [%]',fontsize='x-large')\n",
        "plt.title('Output from epoch: 0 of ' + str(number_of_epochs),fontsize='x-large')\n",
        "plt.show()\n",
        "\n",
        "# Define which loss function we will be using\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Specify the optimizer (or gradient descent algorithm)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Go through each epoch\n",
        "for ep in range(number_of_epochs):\n",
        "\n",
        "  # Set up a progress bar for training\n",
        "  with tqdm(total=len(mnist_data), desc=f'Epoch {ep + 1}/{number_of_epochs}', unit='img') as pbar:\n",
        "    \n",
        "    # Go through all of the images in the dataset\n",
        "    for i, data in enumerate(data_loader, 0):\n",
        "      # Get the current minibatch\n",
        "      inputs, labels = data\n",
        "\n",
        "      if canUseGPU:\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "      # Clear gradients (this is just required)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Perform forward pass\n",
        "      output = model.forward(inputs)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss_value = loss(output,labels)\n",
        "\n",
        "      # Back-propogate\n",
        "      loss_value.backward()\n",
        "\n",
        "      # Gradient descent\n",
        "      optimizer.step()  \n",
        "\n",
        "      # Update progress bar\n",
        "      pbar.update(batchsize)\n",
        "\n",
        "  # Clear the output\n",
        "  clear_output(wait=True)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    test_output = model.forward(test_images).cpu()\n",
        "    test_prob = torch.softmax(test_output,1)*100\n",
        "\n",
        "  # Plot the results\n",
        "  fig = plt.figure(figsize=(10,5))\n",
        "  plt.imshow(test_prob)\n",
        "  ax = plt.gca()\n",
        "  plt.xticks([])\n",
        "  secax = ax.secondary_xaxis('top')\n",
        "  secax.set_xticks(np.arange(0,10))\n",
        "  secax.set_xlabel('Predicted digit',fontsize='x-large')\n",
        "  plt.yticks(np.arange(0,10))\n",
        "  ax.set_ylabel('Actual digit',fontsize='x-large')\n",
        "  plt.clim(0,100)\n",
        "  cbar = plt.colorbar()\n",
        "  cbar.set_label('Confidence in prediction [%]',fontsize='x-large')\n",
        "  plt.title('Output from epoch: '+ str(ep+1) + ' of ' + str(number_of_epochs),fontsize='x-large')\n",
        "  plt.show()\n",
        "\n",
        "  #display_filters(model.conv1.weight)\n",
        "\n",
        "# Show each digit in the small test batch and display the digit predicted with the models confidence\n",
        "f = plt.figure(figsize=(15,15))\n",
        "\n",
        "for i in range(10):\n",
        "  f.add_subplot(2,5,i+1)\n",
        "  plt.imshow(test_images[i,:,:].squeeze(0).cpu())\n",
        "  plt.gca().set_title(\"Predicted: \" + str(torch.argmax(test_prob[i,:]).detach().numpy()) + \" (\" + str(torch.max(test_prob[i,:]).detach().numpy()) + \"%)\")\n",
        "f.subplots_adjust(bottom=0.55)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKkMUbANLMhQ"
      },
      "source": [
        "# Finishing up\n",
        "---\n",
        "##### **_If you used a GPU and are fininshed with the notebook, please make sure to end your session with Google Colab by selecting:_**\n",
        "> ### Runtime > Manage sessions\n",
        "##### **_A window will pop up and you need to locate the current notebook and select:_**\n",
        "> ### TERMINATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08iydcfdhNMA"
      },
      "source": [
        "# Ready for the next notebook?\n",
        "---\n",
        "##### You can click [here](https://colab.research.google.com/github/aewoessn/biomedical-image-analysis-and-ai/blob/main/notebooks/4.1_PredictingAgeFromFaces.ipynb) to take you to the next notebook."
      ]
    }
  ]
}
